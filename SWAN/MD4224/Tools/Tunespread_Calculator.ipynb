{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used to calculate the exact tunespread of a shot from measured data \n",
    "This script uses the tunespread tool and the Mathematica notebook (used to collect data from the tomo dat files) both written by Adrian Oeftiger (CERN BE-ABP-HSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from io import StringIO\n",
    "lorentz_beta = 0.91444281513833\n",
    "lorentz_gamma = 2.4708737618826    \n",
    "from scipy.constants import m_p, m_e, speed_of_light, e\n",
    "r_p = 2.8179403267e-15 *m_e/m_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomo helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables saved in tomoXYZ_eval.dat files in order of line\n",
    "long_eval_headers = [\n",
    "    'matchedemittance', 'ninetyemittance', 'rmsemittance', \n",
    "    'statisticalemittance', 'bunchingfactor', 'fs0', 'fs1', \n",
    "    'eperimage', 'peakcurrent', 'peakdensity', 'sigmaE', \n",
    "    'deltap', 'bunchlength', 'dtbin', 'dEbin', 'phasespace',\n",
    "]\n",
    "\n",
    "def extract_ctime_harmonic(fname, data):\n",
    "    with open(fname + '.dat', 'r') as stream:\n",
    "        stream.readline() # first line\n",
    "        data['time'] = int(stream.readline()) # second line\n",
    "        for i in range(67):\n",
    "            stream.readline()\n",
    "        data['harmonic'] = int(stream.readline()) # 69th line\n",
    "        data['lshape'] = data['bunchingfactor'] #/ data['harmonic']\n",
    "    #print 'harmonic = ', data['harmonic'], ' time of file = ', data['time'], 'lshape = ', data['lshape']\n",
    "    return data\n",
    "\n",
    "def extract_long_eval(ftime):\n",
    "    data = {}\n",
    "    if '_eval.dat' in ftime:\n",
    "        pass\n",
    "    elif '.dat' in ftime:\n",
    "        ftime = ftime[:-4]\n",
    "        ftime = ftime + '_eval.dat'\n",
    "    else:\n",
    "        ftime = ftime + '_eval.dat'\n",
    "        \n",
    "    with open (ftime) as stream :\n",
    "        for header in long_eval_headers[:-1]:\n",
    "            data[header] = float(stream.readline())\n",
    "    extract_ctime_harmonic(ftime[:-9], data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_long_phasespace(ftime):\n",
    "    #with open(ftime + '_tomo{ctime}_eval.dat'.format(ctime=ctime), 'r') as stream:\n",
    "    with open(ftime + '_eval.dat', 'r') as stream:\n",
    "        fcontent = stream.readlines()\n",
    "    phasespace = fcontent[15][2:-2]\n",
    "    phasespace = (phasespace.replace('}, {', '\\n')\n",
    "                            .replace('*^', 'e'))\n",
    "    phasespace = StringIO(unicode(phasespace))\n",
    "    return np.genfromtxt(phasespace, dtype=np.float64, delimiter=',')\n",
    "\n",
    "def extract_deltap_profile(ftime):\n",
    "    #profile = np.genfromtxt(ftime + '_tomo{ctime}_deltap.dat'.format(ctime=ctime), delimiter=',', dtype=np.float64, unpack=True)\n",
    "    profile = np.genfromtxt(ftime + '_deltap.dat', delimiter=',', dtype=np.float64, unpack=True)\n",
    "    return profile\n",
    "\n",
    "def extract_fildeltap_profile(ftime):\n",
    "    #profile = np.genfromtxt(ftime + '_tomo{ctime}_fildeltap.dat'.format(ctime=ctime), delimiter=',', dtype=np.float64, unpack=True'):\n",
    "    profile = np.genfromtxt(ftime + '_fildeltap.dat', delimiter=',', dtype=np.float64, unpack=True)\n",
    "    return profile\n",
    "\n",
    "def extract_intensities(myDataStruct, window_radius_ms=3):\n",
    "    ctime = 185\n",
    "    t_offset = myDataStruct.PR_BCT_ST.Samples.value.firstSampleTime\n",
    "    lo, hi = (ctime - window_radius_ms - t_offset, \n",
    "              ctime + window_radius_ms - t_offset)\n",
    "    intensity_185 = 1e10 * np.mean(myDataStruct.PR_BCT_ST.Samples.value.samples[lo:hi+1])\n",
    "    ctime = 1350\n",
    "    t_offset = myDataStruct.PR_BCT_ST.Samples.value.firstSampleTime\n",
    "    lo, hi = (ctime - window_radius_ms - t_offset, \n",
    "              ctime + window_radius_ms - t_offset)\n",
    "    intensity_1350 = 1e10 * np.mean(myDataStruct.PR_BCT_ST.Samples.value.samples[lo:hi+1])\n",
    "    return intensity_185, intensity_1350\n",
    "\n",
    "def extract_long_profile(myDataStruct_scopechannel):\n",
    "    '''Return time position [in ns] and normalised profile [integrates to 1].'''\n",
    "    dtbin = myDataStruct_scopechannel.Acquisition.value.sampleInterval \n",
    "    tomodata = myDataStruct_scopechannel.Acquisition.value.value\n",
    "    tomotimes = np.arange(0, len(tomodata[0]) * dtbin, dtbin)\n",
    "    tomo_reference = tomodata[0]\n",
    "    baseline = np.mean(tomo_reference[:int(len(tomo_reference)*0.05)])\n",
    "    tomoprofile = tomo_reference - baseline\n",
    "    tomoprofile /= np.trapz(tomoprofile, tomotimes)\n",
    "\n",
    "    return tomotimes, tomoprofile\n",
    "\n",
    "def plot_longphasespace(ftime, ax=None, *contourf_args, **contourf_kwargs):\n",
    "    longdata = extract_long_eval(ftime)\n",
    "    longdata['phasespace'] = extract_long_phasespace(ftime)\n",
    "    elen, tlen = longdata['phasespace'].shape\n",
    "    thalf = tlen*longdata['dtbin']/2.\n",
    "    ehalf = elen*longdata['dEbin']/2.\n",
    "    TT, EE = np.meshgrid(np.linspace(-thalf, thalf, tlen), \n",
    "                         np.linspace(-ehalf, ehalf, elen))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(TT, EE, longdata['phasespace'].T, origin='lower', \n",
    "                cmap=plt.get_cmap('hot_r'),\n",
    "                *contourf_args, **contourf_kwargs)\n",
    "    ax.set_aspect(thalf/ehalf)\n",
    "    plt.grid(True)\n",
    "\n",
    "#long_data = extract_ctime_harmonic('/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_2018_10_22/tomo/2018-10-16_012',long_data)\n",
    "#print long_data\n",
    "#plot_longphasespace('/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_2018_10_22/tomo/2018-10-16_012')\n",
    "\n",
    "\n",
    "def plot_c185_c1350_longphasespace(ftime):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11,5), sharex=True, sharey=True)\n",
    "\n",
    "    plot_longphasespace(ftime[:-4], ctime=185, ax=axes[0])\n",
    "    axes[0].set_title('C185')\n",
    "    plot_longphasespace(ftime[:-4], ctime=1350, ax=axes[1])\n",
    "    axes[1].set_title('C1350')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.grid(True)\n",
    "\n",
    "def plot_c185_c1350(what, avg=True, fig=None):\n",
    "    if fig:\n",
    "        ax = plt.gcf(fig).axes\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12,5), sharex=True, sharey=True)\n",
    "    plt.sca(ax[0])\n",
    "    ax[0].set_title('C185')\n",
    "    ax[0].plot(data_gauss[what][0], label='gauss')\n",
    "    ax[0].plot(data_hollow[what][0], label='hollow')\n",
    "    if avg:\n",
    "        ax[0].axhline(np.mean(data_gauss[what][0]), c='b', ls='--')\n",
    "        ax[0].axhline(np.mean(data_hollow[what][0]), c='g', ls='--')\n",
    "    ax[0].grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.sca(ax[1])\n",
    "    ax[1].set_title('C1350')\n",
    "    ax[1].plot(data_gauss[what][1], label='gauss')\n",
    "    ax[1].plot(data_hollow[what][1], label='hollow')\n",
    "    if avg:\n",
    "        ax[1].axhline(np.mean(data_gauss[what][1]), c='b', ls='--')\n",
    "        ax[1].axhline(np.mean(data_hollow[what][1]), c='g', ls='--')\n",
    "    ax[1].grid(True)\n",
    "    plt.legend()\n",
    "    plt.suptitle(what, fontsize=18)\n",
    "    plt.subplots_adjust(top=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfs_table_return_data(abs_path, beta=0.91444281513833, gamma=2.4708737618826):\n",
    "\n",
    "    madx_file_absolute_path = abs_path\n",
    "    with open(madx_file_absolute_path, 'r') as madx_file:\n",
    "        # New method - search for 'TIME' in file\n",
    "        for line in xrange(80): \n",
    "            if 'TIME' in madx_file.readline():\n",
    "                madx_keywords_line = int(line)+1\n",
    "                break     \n",
    "\n",
    "        # Save MADX keywords as data labels\n",
    "        labels = madx_file.readline()[1:-1]\n",
    "#        print 'read_tfs_table_return_data::', len(labels.split()), ' labels found in MADX TWISS file'\n",
    "\n",
    "        madx_file.seek(madx_keywords_line+1)    \n",
    "\n",
    "        # All we need is S, BETX, BETY, DX, DY, and the Circumference\n",
    "        beta_x_col= -1\n",
    "        beta_y_col= -1\n",
    "        d_x_col = -1\n",
    "        d_y_col = -1\n",
    "        s_col = -1\n",
    "        for l in range(len(labels.split())):\n",
    "            if labels.split()[l] == ('BETX'): beta_x_col = l\n",
    "            if labels.split()[l] == ('BETY'): beta_y_col = l\n",
    "            if labels.split()[l] == ('DX'): d_x_col = l\n",
    "            if labels.split()[l] == ('DY'): d_y_col = l\n",
    "            if labels.split()[l] == ('S'): s_col = l\n",
    "\n",
    "    def file_len(fname):\n",
    "        with open(fname) as f:\n",
    "            for counter, value in enumerate(f):\n",
    "                pass\n",
    "        print '\\n', fname, ' has ', counter+1, ' lines'\n",
    "        return counter + 1\n",
    "\n",
    "    madx_file_lines = file_len(madx_file_absolute_path)\n",
    "\n",
    "    # open twiss file again\n",
    "    with open(madx_file_absolute_path, 'r') as madx_file:\n",
    "        # read past the header lines\n",
    "        for x in range(madx_keywords_line+2):        \n",
    "            madx_file.readline() \n",
    "\n",
    "        circumference = 0.0\n",
    "        remaining_lines = madx_file_lines - (madx_keywords_line + 2)\n",
    "        s = np.empty(remaining_lines, dtype=float)\n",
    "        beta_x = np.empty(remaining_lines, dtype=float)\n",
    "        beta_y = np.empty(remaining_lines, dtype=float)\n",
    "        d_x = np.empty(remaining_lines, dtype=float)\n",
    "        d_y = np.empty(remaining_lines, dtype=float)\n",
    "\n",
    "        # read in data line by line to fill dictionary\n",
    "        for x in range(remaining_lines):   \n",
    "            line = madx_file.readline()\n",
    "            #circumference = circumference + float(line.split()[s_col])    \n",
    "            s[x] = float(line.split()[s_col])    \n",
    "            beta_x[x] = float(line.split()[beta_x_col])\n",
    "            beta_y[x] = float(line.split()[beta_y_col])\n",
    "            d_x[x] = float(line.split()[d_x_col]) * lorentz_beta\n",
    "            d_y[x] = float(line.split()[d_y_col]) * lorentz_beta\n",
    "\n",
    "    # Now we have arrays for betax betay dx dy and a value for the circumference  \n",
    "    # Let's put them in the expected dicitonary\n",
    "    myData = {}\n",
    "\n",
    "    myData['s'] = s\n",
    "    myData['beta_x'] = beta_x\n",
    "    myData['beta_y'] = beta_y\n",
    "    myData['d_x'] = d_x\n",
    "    myData['d_y'] = d_y\n",
    "\n",
    "    # Alternatively we can create a multidimensional numpy array\n",
    "    #twiss_data = np.column_stack((s, beta_x, beta_y, d_x, d_y))\n",
    "\n",
    "#    print 'read_tfs_table_return_data: complete. Returning data dictionary for s, beta_x, beta_y, d_x, and d_y' \n",
    "    return myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tst_inputs(intensity, deltap, emit_geo_x, emit_geo_y, std_x_div_by_Dx, lorentz_beta=0.91444281513833, lorentz_gamma=2.4708737618826, mass=m_p):\n",
    "    return dict(\n",
    "        mass = (mass * speed_of_light**2/e * 1e-9),\n",
    "        beta = lorentz_beta,\n",
    "        gamma = lorentz_gamma,\n",
    "        n_part = int(intensity),\n",
    "        deltap = deltap,\n",
    "        emit_geom_x = emit_geo_x,\n",
    "        emit_geom_y = emit_geo_y,\n",
    "        n_charges_per_part = 1,\n",
    "        std_x_div_by_Dx = std_x_div_by_Dx,\n",
    "    )\n",
    "#make_tst_inputs = np.vectorize(make_tst_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.constants import epsilon_0, c, m_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5346982646272323e-18"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e**2/(4*np.pi*epsilon_0*m_p*c**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5346982671813621e-18"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exact_tunespread(data, inputs, tomodata, beta=0.91444281513833, gamma=2.4708737618826, f_verbose=False):\n",
    "    \"\"\"Calculates the (maximum) tune shift DeltaQ_x and DeltaQ_y due\n",
    "    to space charge with given optics parameters provided by\n",
    "    the dictionaries / hash tables <i>inputs</i> and <i>data</i>.\n",
    "    Assume coasting so we don't need sig_z\n",
    "    \"\"\"\n",
    "#     std_x_div_by_Dx is the horizontal profile's standard deviation\n",
    "#     divided by the horizontal dispersion at that point. \n",
    "#     Equivalently for the vertical plane with std_y_div_by_Dy.\n",
    "    r = r_p\n",
    "    n_part = inputs[\"n_part\"]\n",
    "    emit_x = inputs[\"emit_geom_x\"]\n",
    "    emit_y = inputs[\"emit_geom_y\"]\n",
    "    #lshape = inputs.get(\"lshape\", 1.) #from tomo\n",
    "    lshape = 1 / tomodata['bunchingfactor']\n",
    "    std_x_div_by_Dx = inputs[\"std_x_div_by_Dx\"] #from gaussian fit + optics_file\n",
    "    integx = integy = 0\n",
    "    \n",
    "    ds = np.diff(data[\"s\"])\n",
    "    beta_x = data[\"beta_x\"][:-1] #from optics file\n",
    "    beta_y = data[\"beta_y\"][:-1] #from optics file\n",
    "    d_x = data[\"d_x\"][:-1] #from optics file\n",
    "\n",
    "    # dispersion only scales the convolution (joint PDF of x = x_beta + D_x * dp)\n",
    "    sqx = std_x_div_by_Dx * d_x\n",
    "    sqy = np.sqrt(emit_y * beta_y)\n",
    "    integx = np.sum(beta_x * ds / (sqx * (sqx + sqy)))\n",
    "    integy = np.sum(beta_y * ds / (sqy * (sqx + sqy)))    \n",
    "    \n",
    "    prefactor = r * n_part / (2.0 * np.pi * beta**2 * gamma**3)\n",
    "\n",
    "    circumference = data[\"s\"][-1]\n",
    "    shapefactor = lshape / circumference\n",
    "    #lambda_max_norm = tomodata['peakcurrent'] / (beta * speed_of_light * e) / tomodata['eperimage']\n",
    "    \n",
    "    \n",
    "    print 'integx =', integx, ', integy = ', integy, 'prefactor = ', prefactor, ', shapefactor = ', shapefactor, 'circumference = ', circumference #'lamda_max_norm = ', lambda_max_norm\n",
    "    print 'sig_x = ', sqx[1], 'ig_y = ', sqy[1]\n",
    "    print 'beta_x =', beta_x[1], 'beta_y =', beta_y[1], 'd_x =', d_x[1], 'max(ds) = ', max(ds), 'std_x_div_by_Dx = ', std_x_div_by_Dx\n",
    "\n",
    "    \n",
    "    DeltaQ_x = prefactor * shapefactor * integx\n",
    "    DeltaQ_y = prefactor * shapefactor * integy\n",
    "    \n",
    "#     DeltaQ_x = circumference * lambda_max_norm * prefactor * shapefactor * integx\n",
    "#     DeltaQ_y = circumference * lambda_max_norm * prefactor * shapefactor * integy    \n",
    "    \n",
    "    return (DeltaQ_x, DeltaQ_y)\n",
    "\n",
    "calc_tune_spread = np.vectorize(lambda tst_inputs: calculate_exact_tunespread(tst_data, tst_inputs, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
