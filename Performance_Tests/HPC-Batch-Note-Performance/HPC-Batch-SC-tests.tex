\documentclass[a4paper]{cernatsnote}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{caption}
\usepackage{listings}
\usepackage{parskip}
\usepackage{hyperref}

\usepackage[usenames,dvipsnames]{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{Brown}{cmyk}{0,0.81,1,0.60}
\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{CadetBlue}{cmyk}{0.62,0.57,0.23,0}

\definecolor{amber}{rgb}{1.0, 0.49, 0.0}
\definecolor{azure}{rgb}{0.0, 0.5, 1.0}
\definecolor{brandeisblue}{rgb}{0.0, 0.44, 1.0}
\definecolor{fluorescentpink}{rgb}{1.0, 0.08, 0.58}
\definecolor{lime}{rgb}{0.0, 1.0, 0.0}

\lstset{ 
	backgroundcolor	=	\color{white},  				% choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
	basicstyle		=	\footnotesize,       				% the size of the fonts that are used for the code
	%basicstyle	=	\normalsize,
	breakatwhitespace=	false,       					% sets if automatic breaks should only happen at whitespace
	breaklines		=	true,               				% sets automatic line breaking
	captionpos		=        b,                   				% sets the caption-position to bottom
	commentstyle	=	\color{dkgreen},  				% comment style
	% deletekeywords	=	{...},        					% if you want to delete keywords from the given language
	escapeinside	=	{\%*}{*)},         				% if you want to add LaTeX within your code
	%extendedchar	=	true,              				% lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	frame			=	ltrb,                  				% adds a frame around the code
	framesep		=	5pt,
	identifierstyle	=	\ttfamily \color{black}\bfseries,
	keywordstyle	=	\ttfamily \color{blue},      		% keyword style
	language		=	Bash,                				% the language of the code
	morekeywords	=	{*,...},           				% if you want to add more keywords to the set
	numbers			=	left,                   				% where to put the line-numbers; possible values are (none, left, right)
	numbersep		=	5pt,                  				% how far the line-numbers are from the code
	numberstyle		=	\tiny\color{gray},  				% the style that is used for the line-numbers
	rulecolor		=	\color{black},        				% if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces		=	false,               				% show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces	=	true,         					% underline spaces within strings only
	showtabs		=	false,                 				% show tabs within strings adding particular underscores
	stepnumber		=	2,                  				% the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle		=	\ttfamily\color{gray},      		% string literal style
	tabsize			=	2,                      				% sets default tabsize to 2 spaces
	title			=	\lstname                  				% show the filename of files included with \lstinputlisting; also try caption instead of title
}


\email{haroon.rafique@cern.ch}

\title{PyORBIT on HPC-Batch}
\documentlabel{CERN-ATS-Note-2018-??? TECH}

\author{Haroon Rafique / BE-ABP}
\keywords{PyORBIT, Space Charge, HPC, HPC-Batch, Slurm, PTC, MAD, Python}
\makeindex

\def \howtoaccess {\url{https://cern.service-now.com/service-portal/article.do?n=KB0004541}}
\def \headnode {\texttt{hpc-batch.cern.ch}}
\def \jbgithub {\href{https://github.com/jbcern/py-orbit}{https://github.com/jbcern/py-orbit.git}}
\def \hbgithub {\href{https://github.com/hannes-bartosik/py-orbit}{https://github.com/hannes-bartosik/py-orbit.git}}
\def \pyorbitgithub {\href{https://github.com/PyORBIT-Collaboration/py-orbit}{https://github.com/PyORBIT-Collaboration/py-orbit.git}}
\def \ptcgithub {\href{https://github.com/jbcern/PTC/tree/analytical-space-charge-acceleration}{https://github.com/jbcern/PTC.git}}
\def \batchroot {\texttt{\textbackslash hpcscratch\textbackslash $<$user$>$\textbackslash}}
\def \batchorbitroot {\texttt{\textbackslash hpcscratch\textbackslash $<$user$>$\textbackslash PyORBIT\textbackslash}}
\def \batchorbitptc {\texttt{\textbackslash hpcscratch\textbackslash $<$user$>$\textbackslash PyORBIT\textbackslash py-orbit\textbackslash ext\textbackslash PTC\textbackslash}}
\def \pyorbiteos {\texttt{\textbackslash eos\textbackslash project\textbackslash p\textbackslash pyorbit\textbackslash public\textbackslash HPC-Batch\textbackslash}}

\begin{document}
	
	\maketitle % this produces the title block
	
	\begin{abstract}
		A batch HPC cluster using Slurm has been set up for users of MPI applications that cannot run on a single node on the regular batch service. Access is restricted to approved HPC users, mainly from the Accelerator and Technology sector. The system uses the Slurm workload manager.
		
		PyORBIT is well suited to this system. Instructions are provided for installing and running PyORBIT on the HPC-Batch system.
	\end{abstract}
	
	\section{Introduction}
	\label{sec:intro}
		
	Official instructions for accessing the HPC-Batch system are available in the knowledge base article \href{https://cern.service-now.com/service-portal/article.do?n=KB0004541}{KB0004541}: How to access and use the batch HPC cluster:
	
	\texttt{\howtoaccess}
	
	These instructions are repeated and built upon here. 
	
	\subsection{Hardware}
	
	\begin{table}
		\begin{center}
			\begin{tabular}[!b]{|l|c|c|}
				\hline
				\textbf{Queue} 			& \textbf{Nodes} & \textbf{Time Limit} \\
				\hline
				\textbf{batch-short}	& 10			  & 2 days (2-00:00:00) \\
				\textbf{batch-long}		& 66			  & $\infty$ \\
				\textbf{be-short}		& 69			  & 2 days (2-00:00:00) \\
				\textbf{be-long}		& 72			  & $\infty$ \\
				\hline
			\end{tabular}
			\caption{Computing resources and job time limits on HPC-Batch.}
			\label{tab:cpus}
		\end{center}
	\end{table}

	Table~\ref{tab:cpus} shows the available resources on HPC-Batch. There are a total of 217 nodes, each with 40 CPUs, totalling 8680 cores. The nodes are split into four queues, two short (2 day limit), and two long (infinite limit). The `batch' queues may be used by all, and the `be' queues are reserved for members of BE (beams department).
	
	HPC-Batch should not replace HTCondor or standard lxplus simulations, instead it should be used for computationally intense simulations where required.
	
	Unfortunately it is not possible to call simulation input files, or in fact executables, from AFS in a reliable manner. Instead one must install PyORBIT on ones HPC-Batch scratch space \texttt{\textbackslash hpcscratch \textbackslash $<$user$>$}, and copy all simulation data to this local directory. When the simulation is complete, it is recommended that the user copies all important output back to AFS or to EOS, as the scratch space is not backed up. This cannot be done within the submission script, and must be done manually, or in a separate script called by the user (i.e. not called from within the submission script).
		
	\subsection{Accessing HPC-Batch}
	\label{sec:access}
	
	In order to gain access to the HPC-Batch system please request access to the e-group \href{https://e-groups.cern.ch/e-groups/Egroup.do?egroupId=10245625&AI_USERNAME=HARAFIQU&searchField=0&searchMethod=0&searchValue=service-hpc-be&pageSize=30&hideSearchFields=false&searchMemberOnly=false&searchAdminOnly=false&AI_SESSION=lKFyHz8s-iVcgi9UtzwK-aBtk4OSxot37tlEfn8KCeHPzfMElLZd!393472965!1522333073243}{service-hpc-be} via e-groups or email \texttt{abp-cwg-admin@cern.ch}.
	
	To access HPC-Batch one must log in to the head node \headnode, and replace $<$user$>$ with ones CERN account user name:
	
	\begin{lstlisting}[language=bash]
	$ ssh -XY <user>@hpc-batch.cern.ch
	\end{lstlisting}
	
	To check the available modules use:
	\begin{lstlisting}[language=bash]
	$ module avail
	\end{lstlisting}
	
	The system use GCC v4 as default, this is sufficient for PyORBIT, but it can be changed to GCC v6 if required using:
	\begin{lstlisting}[language=bash]
	$ module load compiler/gcc6
	\end{lstlisting}	
	
	MPI versions may also be selected, for PyORBIT, and in general, one must initialise this using:
	
	\begin{lstlisting}[language=bash]
	$ module load mpi/mvapich2/2.2
	\end{lstlisting}
	
	This may be included in ones \texttt{ $\sim$ /.bashrc} configuration file, or ones Slurm submission script.
	
	\section{Installing PyORBIT on HPC-Batch}
	\label{sec:install}
	
	There are currently a number of options available when installing PyORBIT. The CERN branches are made available on GitHub by Hannes Bartosik and Jean-Baptiste Lagrange. The original code (developed by SNS) is also available on GitHub. Two options are provided, the first is using Jean-Baptiste Lagrange's versions of PyORBIT and PTC, and the second is replacing the PyORBIT version with one of Hannes Bartosik's branches.
	
	Jean-Baptiste Lagrange's code is located in the public repository:
	
	\texttt{\jbgithub}
	
	Hannes Bartosik's code is located in the public repository:
	
	\texttt{\hbgithub}
	
	please note that there are a number of branches; \href{https://github.com/hannes-bartosik/py-orbit/tree/new-features}{new-features} and \href{https://github.com/hannes-bartosik/py-orbit/tree/analytical-space-charge}{analytic-space-charge} are likely to be the most useful currently.
	
	These branches will all be merged in the near future.
	
	The original PyORBIT code is also available from GitHub, but may not provide the required functionality for CERN users:
	
	\texttt{\pyorbitgithub}
	
	For tracking, PTC is also required. The latest version to be used for PyORBIT can be found on GitHub:
	
	\texttt{\ptcgithub}
	
	The \href{https://github.com/jbcern/PTC/tree/analytical-space-charge-acceleration}{analytical-space-charge-acceleration} branch provides the most up-to-date version.
	
	PyORBIT runs within a virtual environment in order to utilise specific versions of libraries such as Python 2.7. As such one must take care of environment variables when running on HPC-Batch.
	
	The user must locally install PyORBIT on HPC-Batch, for this step-by-step instructions are given (if facing difficulties please contact haroon.rafique, or py.orbit at cern.ch):
	
	\begin{enumerate}
		\item Log into the HPC-Batch head node:
			\begin{lstlisting}[language=bash]
			$ ssh -XY <user>@hpc-batch.cern.ch
			\end{lstlisting}
		\item Make sure that your \texttt{\$PATH} and other environment variables are clear in your \texttt{$\sim$/.bashrc} configuration file.
		\item Use the PyORBIT installation script `install\_PyORBIT.sh' given in subsection~\ref{sec:install_script}. Create this file and copy the contents, making sure to adjust the variables as instructed in subsection~\ref{sec:install_script}.
		\item Create a directory in your home space \batchroot (it is suggested to name this directory \texttt{PyORBIT} if you only intend to install a single version of PyORBIT and PTC, otherwise name the directory something else - for example \texttt{PyORBIT\_new\_features} and create a soft link using linux command \texttt{ln -s link\_name directory\_name} called PyORBIT). Enter this directory, copy the install script here, and run it:
			\begin{lstlisting}[language=bash]
			$ ./install_PyORBIT.sh > install_output.txt &
			\end{lstlisting}
		\item The installation script should take a few hours to run. It is difficult to know when it has finished as all output will be pipelined to the \texttt{`install\_output.txt'} file. Keep checking when this file has been updated, if it is within the last few minutes then the install script is still running.
		\item After a few hours check that the install script has finished running, and check the \texttt{`install\_output.txt'} file in order to ascertain if the installation proceeded with no errors.
		\item If no errors are present, it is usually necessary to install PTC manually. Though the install script should have pulled PTC from GitHub, it may not have been installed due to an environment variable in an included macro file. Check the directory \batchorbitptc \texttt{\textbackslash obj}, if there are no \texttt{.o} files present, PTC has not been installed.
		
		In this case run the \texttt{make} command in \batchorbitptc. It is likely that this will produce the following error:
		\begin{lstlisting}[language=bash]
			../../conf/make_root_config:5: /conf/make_common_config: No such file or directory
			../../conf/make_root_config:25: /conf//make_root_config: No such file or directory
			make: *** No rule to make target `/conf//make_root_config'. Stop.			
		\end{lstlisting}		
		In order to mitigate this, in the 
		
		\batchorbitroot\texttt{\textbackslash py-orbit\textbackslash conf\textbackslash make\_root\_config} 
		
		file, comment out lines 5 and 25 by placing a \texttt{\#} at the start of each line:
		\begin{lstlisting}[language=bash]
			include  ${ORBIT_ROOT}/conf/${ORBIT_ARCH}/make_root_config
			...
			include  ${ORBIT_ROOT}/conf/make_common_config
		\end{lstlisting}
		The intel fortran libraries are required to install PTC, so one must also execute the following source command:
		\begin{lstlisting}[language=bash]
			$ source /cvmfs/projects.cern.ch/intelsw/psxe/linux/all-setup.sh
		\end{lstlisting}
		
		The Makefile in the \texttt{PTC} directory should also be modified:
		\begin{lstlisting}[language=bash]		
		#-------------------------------------------------------------------------------
		# External 'include' locations
		#-------------------------------------------------------------------------------
		
		INCLUDES += -I/hpcscratch/user/fasvesta/PyORBIT/include/python2.7
		\end{lstlisting}		
		
		Now executing the \texttt{make} command in \batchorbitptc should be successful. After installing PTC (check the \batchorbitptc \texttt{\textbackslash obj} for \texttt{.o} files as before), remember to uncomment out the lines in the configuration file.
		
		\item[]{Once these steps are complete, PyORBIT has been installed.}		
		
	\end{enumerate}
	
	
	
	
	
	\section{Job Submission}
	
	Log in to head node (not where things are run).
	
	Using the correct queue
	
	\subsection{Slurm Commands}
	
	\begin{itemize}
		\item{\textbf{squeue}} - view job and job step information, -u $<$user$>$ displays a specific users jobs.
		\item{\textbf{sacct}} - displays accounting data for all obs and job steps in the Slurm accounting log or Slurm database.
		\item{\textbf{sbatch}} - submit a batch script to Slurm.
		\item{\textbf{srun}} - run parallel jobs (used similarly to mpirun).
		\item{\textbf{scancel}} - used with a job ID to kill the specified job.
	\end{itemize}

	\subsection{Slurm Script Commands}
	
	In a bash script, the following \texttt{\#SBATCH} commands may be used to specify job parameters:
	
	\begin{itemize}
		\item{\textbf{-p}} - queue name, for HPC-Batch the options are be-short, be-long, batch-short, batch-long.
		\item{\textbf{-N}} - number of nodes, each node has 40 cpus on HPC-Batch, and the number of nodes does not need to be specified, only the number of CPUs.
		\item{\textbf{-n}} - number of cpus.
		\item{\textbf{-t}} - expected wall time for the job, in the format day-hour:minute:second, for example 1-10:00:00. This is a hard limit, if exceeded your job will be cancelled by Slurm.
		\item{\textbf{-o}} - the output file created (not from PyORBIT). This file will record what would normally be output to screen/console. A useful format is name.\%N.\%j.out, where \% is the node number, and \%j is the Job ID.
		\item{\textbf{-e}} - the error file created (not from PyORBIT). This file will record what would normally be output to screen/console. A useful format is name.\%N.\%j.out, where \% is the node number, and \%j is the Job ID.
		\item{\textbf{--mem}} - the RAM allocation for this job, for example 10gb.		
	\end{itemize}

An example of the use of these variables is shown below:
	
	\begin{lstlisting}[language=bash]
	#!/bin/bash
	#SBATCH -p be-short
	#SBATCH -n 100
	#SBATCH --mem 10gb
	#SBATCH -t 1-10:00
	#SBATCH -o slurm.%N.%j.out
	#SBATCH -e slurm.%N.%j.err
	\end{lstlisting}
	
	\section{Scripts}
	\label{sec:scripts}
	
	A number of scripts are provided as examples or otherwise.
	
	\subsection{HPC-Batch Custom Environment}
	\label{sec:environment_script}
	
	The following environment variable may be used independently, called from within a Slurm submission script, or incorporated into a submission script in order to setup important environment variables.
	
	\begin{lstlisting}[language=bash]
	#!/bin/bash
	# Use the installed Python version 2.7 (required for PyORBIT)
	source /hpcscratch/user/<user>/PyORBIT/virtualenvs/py2.7/bin/activate
	
	# Set up the PyORBIT environment
	source /hpcscratch/user/<user>/PyORBIT/py-orbit/customEnvironment.sh
	
	# Intel fortran libraries needed in PTC
	source /cvmfs/projects.cern.ch/intelsw/psxe/linux/all-setup.sh
	\end{lstlisting}
	
	\subsection{PyORBIT Installation}
	\label{sec:install_script}
	
	The install script `install\_PyORBIT.sh' may be used to install PyORBIT on the HPC-Batch cluster. It is provided here but may also be found here:
	
	\pyorbiteos
	
	\begin{lstlisting}[language=bash]
	#!/bin/bash
	
	#######################################################################
	#   Script to build PTC-pyORBIT from Source with a custom Environment
	#   from JB Lagrange Github depositories for pyORBIT and PTC
	#   Use of this script:
	#   1) create a folder for the whole environment (ex:pyorbit_env)
	#   2) copy this script in this folder
	#   3) execute this script in this folder
	#   After installing everything, you can check things by running the examples in py-orbit/examples/
	#
	#   NB: if you want to recompile pyORBIT after the first installation, you need to use:
	#	cd py-orbit
	#	source /cvmfs/projects.cern.ch/intelsw/psxe/linux/all-setup.sh
	#	source customEnvironment.sh
	#	make clean
	#	make
	#######################################################################
	
	#source ifort for compiling PTC
	source /cvmfs/projects.cern.ch/intelsw/psxe/linux/all-setup.sh
	
	#clone pyorbit version from github:
	#~ git clone --branch=smooth_binning https://github.com/jbcern/py-orbit.git
	git clone --branch=new-features https://github.com/hannes-bartosik/py-orbit.git
	
	#clone PTC from github
	cd py-orbit/ext
	git clone --branch=analytical-space-charge https://github.com/jbcern/PTC.git
	cd PTC
	mkdir obj/
	cd ../../..
	
	#download and untar sources
	echo "download and untar sources..."
	curl http://www.mpich.org/static/downloads/3.2/mpich-3.2.tar.gz | tar xvz
	curl https://www.python.org/ftp/python/2.7.12/Python-2.7.12.tgz | tar xvz
	curl http://zlib.net/fossils/zlib-1.2.11.tar.gz | tar xvz
	curl http://www.fftw.org/fftw-3.3.5.tar.gz | tar xvz
	curl https://pypi.python.org/packages/source/v/virtualenv/virtualenv-15.0.0.tar.gz  | tar xvz
	
	#build python
	echo "build python2.7..."
	cd Python-2.7.12
	./configure -prefix=`pwd`/..
	make
	make install
	cd ..
	
	#build zlib
	echo "build zlib..."
	cd zlib-1.2.11
	./configure -prefix=`pwd`/..
	make
	make install
	cd ..
	
	#build mpi
	echo "build mpich..."
	cd mpich-3.2
	./configure -prefix=`pwd`/.. --disable-fortran
	make
	make install
	cd ..
	
	#build fftw
	echo "build fftw..."
	cd fftw-3.3.5
	./configure -prefix=`pwd`/.. --disable-fortran --enable-mpi MPICC=`pwd`/../bin/mpicc
	make
	make install
	cd ..
	
	#build python packages
	echo "build python packages..."
	source py-orbit/customEnvironment.sh
	cd virtualenv-15.0.0
	../bin/python setup.py install
	
	cd ..
	mkdir virtualenvs
	cd virtualenvs
	../bin/virtualenv py2.7 --python=../bin/python
	cd py2.7/bin
	source activate
	
	#Add here the python packages you want to install
	echo "installing numpy..."
	./pip install numpy
	echo "installing scipy..."
	./pip install scipy
	echo "installing ipython..."
	./pip install ipython
	echo "installing matplotlib..."
	./pip install matplotlib
	echo "installing h5py..."
	./pip install h5py
	echo "DONE"
	echo
	cd ../../..
	
	#build pyorbit
	echo "Building pyORBIT..."
	cd py-orbit
	source customEnvironment.sh
	make clean
	make
	
	
	\end{lstlisting}
	
	\newpage

	\section{Run Times}
	Standard test case 3000 macro particles, $10^{5}$ turns, PS, frozen space charge.
	
	\begin{figure}		
		\centering
		\includegraphics[width=0.8\columnwidth]{Runtimes_log.png}
		\caption{Run time per CPU for the test case.}
		\label{fig:runtime}
	\end{figure}

	\begin{figure}		
		\centering
		\includegraphics[width=0.8\columnwidth]{Speedup_per_cpu.png}
		\caption{Speedup for the test case.}
		\label{fig:speedup}
	\end{figure}

Peak speedup at around 30 macro-particles per core. Expect due to bottleneck as number of macro particles is small.
	
\end{document}
