{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used to calculate the exact tunespread of a shot from measured data \n",
    "This script uses the tunespread tool and the Mathematica notebook (used to collect data from the tomo dat fileS) both written by Adrian Oeftiger (CERN BE-ABP-HSC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from io import StringIO\n",
    "lorentz_beta = 0.91444281513833\n",
    "lorentz_gamma = 2.4708737618826    \n",
    "from scipy.constants import m_p, m_e, speed_of_light, e\n",
    "r_p = 2.8179403267e-15 *m_e/m_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tomo helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variables saved in tomoXYZ_eval.dat files in order of line\n",
    "long_eval_headers = [\n",
    "    'matchedemittance', 'ninetyemittance', 'rmsemittance', \n",
    "    'statisticalemittance', 'bunchingfactor', 'fs0', 'fs1', \n",
    "    'eperimage', 'peakcurrent', 'peakdensity', 'sigmaE', \n",
    "    'deltap', 'bunchlength', 'dtbin', 'dEbin', 'phasespace',\n",
    "]\n",
    "\n",
    "def extract_ctime_harmonic(fname, data):\n",
    "    if '_eval.dat' in fname:\n",
    "        fname = fname[:-9]\n",
    "    else:\n",
    "        pass\n",
    "    with open(fname + '.dat', 'r') as stream:\n",
    "        stream.readline() # first line\n",
    "        data['time'] = int(stream.readline()) # second line\n",
    "        for i in range(67):\n",
    "            stream.readline()\n",
    "        data['harmonic'] = int(stream.readline()) # 69th line\n",
    "        data['lshape'] = data['bunchingfactor'] / data['harmonic']\n",
    "    print 'harmonic = ', data['harmonic'], ' time of file = ', data['time'], 'lshape = ', data['lshape']\n",
    "    return data\n",
    "\n",
    "def extract_long_eval(ftime):\n",
    "    data = {}\n",
    "    if '_eval.dat' in ftime:\n",
    "        pass\n",
    "    else:\n",
    "        ftime = ftime + '_eval.dat'\n",
    "        \n",
    "    with open (ftime) as stream :\n",
    "        for header in long_eval_headers[:-1]:\n",
    "            data[header] = float(stream.readline())\n",
    "    extract_ctime_harmonic(ftime, data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_long_phasespace(ftime):\n",
    "    #with open(ftime + '_tomo{ctime}_eval.dat'.format(ctime=ctime), 'r') as stream:\n",
    "    with open(ftime + '_eval.dat', 'r') as stream:\n",
    "        fcontent = stream.readlines()\n",
    "    phasespace = fcontent[15][2:-2]\n",
    "    phasespace = (phasespace.replace('}, {', '\\n')\n",
    "                            .replace('*^', 'e'))\n",
    "    phasespace = StringIO(unicode(phasespace))\n",
    "    return np.genfromtxt(phasespace, dtype=np.float64, delimiter=',')\n",
    "\n",
    "def extract_deltap_profile(ftime):\n",
    "    #profile = np.genfromtxt(ftime + '_tomo{ctime}_deltap.dat'.format(ctime=ctime), delimiter=',', dtype=np.float64, unpack=True)\n",
    "    profile = np.genfromtxt(ftime + '_deltap.dat', delimiter=',', dtype=np.float64, unpack=True)\n",
    "    return profile\n",
    "\n",
    "def extract_fildeltap_profile(ftime):\n",
    "    #profile = np.genfromtxt(ftime + '_tomo{ctime}_fildeltap.dat'.format(ctime=ctime), delimiter=',', dtype=np.float64, unpack=True'):\n",
    "    profile = np.genfromtxt(ftime + '_fildeltap.dat', delimiter=',', dtype=np.float64, unpack=True)\n",
    "    return profile\n",
    "\n",
    "def extract_intensities(myDataStruct, window_radius_ms=3):\n",
    "    ctime = 185\n",
    "    t_offset = myDataStruct.PR_BCT_ST.Samples.value.firstSampleTime\n",
    "    lo, hi = (ctime - window_radius_ms - t_offset, \n",
    "              ctime + window_radius_ms - t_offset)\n",
    "    intensity_185 = 1e10 * np.mean(myDataStruct.PR_BCT_ST.Samples.value.samples[lo:hi+1])\n",
    "    ctime = 1350\n",
    "    t_offset = myDataStruct.PR_BCT_ST.Samples.value.firstSampleTime\n",
    "    lo, hi = (ctime - window_radius_ms - t_offset, \n",
    "              ctime + window_radius_ms - t_offset)\n",
    "    intensity_1350 = 1e10 * np.mean(myDataStruct.PR_BCT_ST.Samples.value.samples[lo:hi+1])\n",
    "    return intensity_185, intensity_1350\n",
    "\n",
    "def extract_long_profile(myDataStruct_scopechannel):\n",
    "    '''Return time position [in ns] and normalised profile [integrates to 1].'''\n",
    "    dtbin = myDataStruct_scopechannel.Acquisition.value.sampleInterval \n",
    "    tomodata = myDataStruct_scopechannel.Acquisition.value.value\n",
    "    tomotimes = np.arange(0, len(tomodata[0]) * dtbin, dtbin)\n",
    "    tomo_reference = tomodata[0]\n",
    "    baseline = np.mean(tomo_reference[:int(len(tomo_reference)*0.05)])\n",
    "    tomoprofile = tomo_reference - baseline\n",
    "    tomoprofile /= np.trapz(tomoprofile, tomotimes)\n",
    "\n",
    "    return tomotimes, tomoprofile\n",
    "\n",
    "def plot_longphasespace(ftime, ax=None, *contourf_args, **contourf_kwargs):\n",
    "    longdata = extract_long_eval(ftime)\n",
    "    longdata['phasespace'] = extract_long_phasespace(ftime)\n",
    "    elen, tlen = longdata['phasespace'].shape\n",
    "    thalf = tlen*longdata['dtbin']/2.\n",
    "    ehalf = elen*longdata['dEbin']/2.\n",
    "    TT, EE = np.meshgrid(np.linspace(-thalf, thalf, tlen), \n",
    "                         np.linspace(-ehalf, ehalf, elen))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(TT, EE, longdata['phasespace'].T, origin='lower', \n",
    "                cmap=plt.get_cmap('hot_r'),\n",
    "                *contourf_args, **contourf_kwargs)\n",
    "    ax.set_aspect(thalf/ehalf)\n",
    "    plt.grid(True)\n",
    "\n",
    "#long_data = extract_ctime_harmonic('/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_2018_10_22/tomo/2018-10-16_012',long_data)\n",
    "#print long_data\n",
    "#plot_longphasespace('/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_2018_10_22/tomo/2018-10-16_012')\n",
    "\n",
    "\n",
    "def plot_c185_c1350_longphasespace(ftime):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11,5), sharex=True, sharey=True)\n",
    "\n",
    "    plot_longphasespace(ftime[:-4], ctime=185, ax=axes[0])\n",
    "    axes[0].set_title('C185')\n",
    "    plot_longphasespace(ftime[:-4], ctime=1350, ax=axes[1])\n",
    "    axes[1].set_title('C1350')\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.grid(True)\n",
    "\n",
    "def plot_c185_c1350(what, avg=True, fig=None):\n",
    "    if fig:\n",
    "        ax = plt.gcf(fig).axes\n",
    "    else:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12,5), sharex=True, sharey=True)\n",
    "    plt.sca(ax[0])\n",
    "    ax[0].set_title('C185')\n",
    "    ax[0].plot(data_gauss[what][0], label='gauss')\n",
    "    ax[0].plot(data_hollow[what][0], label='hollow')\n",
    "    if avg:\n",
    "        ax[0].axhline(np.mean(data_gauss[what][0]), c='b', ls='--')\n",
    "        ax[0].axhline(np.mean(data_hollow[what][0]), c='g', ls='--')\n",
    "    ax[0].grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.sca(ax[1])\n",
    "    ax[1].set_title('C1350')\n",
    "    ax[1].plot(data_gauss[what][1], label='gauss')\n",
    "    ax[1].plot(data_hollow[what][1], label='hollow')\n",
    "    if avg:\n",
    "        ax[1].axhline(np.mean(data_gauss[what][1]), c='b', ls='--')\n",
    "        ax[1].axhline(np.mean(data_hollow[what][1]), c='g', ls='--')\n",
    "    ax[1].grid(True)\n",
    "    plt.legend()\n",
    "    plt.suptitle(what, fontsize=18)\n",
    "    plt.subplots_adjust(top=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfs_table_return_data(abs_path, beta=0.91444281513833, gamma=2.4708737618826):\n",
    "\n",
    "    madx_file_absolute_path = abs_path\n",
    "    with open(madx_file_absolute_path, 'r') as madx_file:\n",
    "        # New method - search for 'TIME' in file\n",
    "        for line in xrange(80): \n",
    "            if 'TIME' in madx_file.readline():\n",
    "                madx_keywords_line = int(line)+1\n",
    "                break     \n",
    "\n",
    "        # Save MADX keywords as data labels\n",
    "        labels = madx_file.readline()[1:-1]\n",
    "        print 'read_tfs_table_return_data::', len(labels.split()), ' labels found in MADX TWISS file'\n",
    "\n",
    "        madx_file.seek(madx_keywords_line+1)    \n",
    "\n",
    "        # All we need is S, BETX, BETY, DX, DY, and the Circumference\n",
    "        beta_x_col= -1\n",
    "        beta_y_col= -1\n",
    "        d_x_col = -1\n",
    "        d_y_col = -1\n",
    "        s_col = -1\n",
    "        for l in range(len(labels.split())):\n",
    "            if labels.split()[l] == ('BETX'): beta_x_col = l\n",
    "            if labels.split()[l] == ('BETY'): beta_y_col = l\n",
    "            if labels.split()[l] == ('DX'): d_x_col = l\n",
    "            if labels.split()[l] == ('DY'): d_y_col = l\n",
    "            if labels.split()[l] == ('S'): s_col = l\n",
    "\n",
    "    def file_len(fname):\n",
    "        with open(fname) as f:\n",
    "            for counter, value in enumerate(f):\n",
    "                pass\n",
    "        print fname, ' has ', counter+1, ' lines'\n",
    "        return counter + 1\n",
    "\n",
    "    madx_file_lines = file_len(madx_file_absolute_path)\n",
    "\n",
    "    # open twiss file again\n",
    "    with open(madx_file_absolute_path, 'r') as madx_file:\n",
    "        # read past the header lines\n",
    "        for x in range(madx_keywords_line+2):        \n",
    "            madx_file.readline() \n",
    "\n",
    "        circumference = 0.0\n",
    "        remaining_lines = madx_file_lines - (madx_keywords_line + 2)\n",
    "        s = np.empty(remaining_lines, dtype=float)\n",
    "        beta_x = np.empty(remaining_lines, dtype=float)\n",
    "        beta_y = np.empty(remaining_lines, dtype=float)\n",
    "        d_x = np.empty(remaining_lines, dtype=float)\n",
    "        d_y = np.empty(remaining_lines, dtype=float)\n",
    "\n",
    "        # read in data line by line to fill dictionary\n",
    "        for x in range(remaining_lines):   \n",
    "            line = madx_file.readline()\n",
    "            #circumference = circumference + float(line.split()[s_col])    \n",
    "            s[x] = float(line.split()[s_col])    \n",
    "            beta_x[x] = float(line.split()[beta_x_col])\n",
    "            beta_y[x] = float(line.split()[beta_y_col])\n",
    "            d_x[x] = float(line.split()[d_x_col]) * lorentz_beta\n",
    "            d_y[x] = float(line.split()[d_y_col]) * lorentz_beta\n",
    "\n",
    "    # Now we have arrays for betax betay dx dy and a value for the circumference  \n",
    "    # Let's put them in the expected dicitonary\n",
    "    myData = {}\n",
    "\n",
    "    myData['s'] = s\n",
    "    myData['beta_x'] = beta_x\n",
    "    myData['beta_y'] = beta_y\n",
    "    myData['d_x'] = d_x\n",
    "    myData['d_y'] = d_y\n",
    "\n",
    "    # Alternatively we can create a multidimensional numpy array\n",
    "    #twiss_data = np.column_stack((s, beta_x, beta_y, d_x, d_y))\n",
    "\n",
    "    print 'read_tfs_table_return_data: complete. Returning data dictionary for s, beta_x, beta_y, d_x, and d_y' \n",
    "    return myData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tst_inputs(intensity, deltap, emit_geo_x, emit_geo_y, sig_z, std_x_div_by_Dx, coasting = 0, lorentz_beta=0.91444281513833, lorentz_gamma=2.4708737618826, mass=m_p):\n",
    "    return dict(\n",
    "        mass = (mass * speed_of_light**2/e * 1e-9),\n",
    "        beta = lorentz_beta,\n",
    "        gamma = lorentz_gamma,\n",
    "        sig_z = sig_z,\n",
    "        n_part = intensity,\n",
    "        deltap = deltap,\n",
    "        emit_geom_x = emit_geo_x,\n",
    "        emit_geom_y = emit_geo_y,\n",
    "        coasting = coasting,\n",
    "        n_charges_per_part = 1,\n",
    "        std_x_div_by_Dx = std_x_div_by_Dx,\n",
    "    )\n",
    "make_tst_inputs = np.vectorize(make_tst_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_tfs_table_return_data:: 256  labels found in MADX TWISS file\n",
      "/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_Twiss/twiss_6p305_6p11.tfs  has  3142  lines\n",
      "read_tfs_table_return_data: complete. Returning data dictionary for s, beta_x, beta_y, d_x, and d_y\n",
      "harmonic =  9  time of file =  1540213900 lshape =  0.0291027162629\n"
     ]
    }
   ],
   "source": [
    "# Read data from TFS table:\n",
    "myData = read_tfs_table_return_data('/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_Twiss/twiss_6p305_6p11.tfs')\n",
    "\n",
    "# Read data from Tomo:\n",
    "long_data = extract_long_eval('/eos/user/h/harafiqu/SWAN_projects/PS/MD4224_2018_10_22/tomo/2018-10-16_012')\n",
    "\n",
    "# Need to provide these:\n",
    "emit_x = 1.\n",
    "emit_y = 1.\n",
    "sig_z = (long_data['bunchlength'] * 1E-9) / (speed_of_light * lorentz_beta) # bunch length in ns? -> sig_z in metres\n",
    "std_x = 1.\n",
    "\n",
    "inputs = make_tst_inputs(55E10, float(long_data['deltap']), emit_x, emit_y, sig_z, std_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_exact_tunespread(data, inputs, beta=0.91444281513833, gamma=2.4708737618826, f_verbose=False):\n",
    "    \"\"\"Calculates the (maximum) tune shift DeltaQ_x and DeltaQ_y due\n",
    "    to space charge with given optics parameters provided by\n",
    "    the dictionaries / hash tables <i>inputs</i> and <i>data</i>.\n",
    "    \"\"\"\n",
    "#     std_x_div_by_Dx is the horizontal profile's standard deviation\n",
    "#     divided by the horizontal dispersion at that point. \n",
    "#     Equivalently for the vertical plane with std_y_div_by_Dy.\n",
    "    r = r_p\n",
    "    #beta = inputs[\"beta\"]\n",
    "    #gamma = inputs[\"gamma\"]\n",
    "    n_part = inputs[\"n_part\"]\n",
    "    emit_x = inputs[\"emit_geom_x\"]\n",
    "    emit_y = inputs[\"emit_geom_y\"]\n",
    "    lshape = inputs.get(\"lshape\", 1.) #from tomo\n",
    "    std_x_div_by_Dx = inputs[\"std_x_div_by_Dx\"] #from gaussian fit + optics_file\n",
    "    integx = integy = 0\n",
    "    \n",
    "    ds = np.diff(data[\"s\"])\n",
    "    beta_x = data[\"beta_x\"][:-1] #from optics file\n",
    "    beta_y = data[\"beta_y\"][:-1] #from optics file\n",
    "    d_x = data[\"d_x\"][:-1] #from optics file\n",
    "\n",
    "    # dispersion only scales the convolution (joint PDF of x = x_beta + D_x * dp)\n",
    "    sqx = std_x_div_by_Dx * d_x\n",
    "    sqy = np.sqrt(emit_y * beta_y)\n",
    "    integx = np.sum(beta_x * ds / (sqx * (sqx + sqy)))\n",
    "    integy = np.sum(beta_y * ds / (sqy * (sqx + sqy)))\n",
    "    \n",
    "    prefactor = r * n_part / (2.0 * np.pi * beta**2 * gamma**3)\n",
    "    if inputs.get('coasting', False):\n",
    "        circumference = data[\"s\"][-1]\n",
    "        shapefactor = lshape / circumference\n",
    "    else:\n",
    "        # assume Gaussian shape\n",
    "        shapefactor = lshape / (np.sqrt(2.0 * np.pi) * inputs[\"sig_z\"])\n",
    "    DeltaQ_x = prefactor * shapefactor * integx\n",
    "    DeltaQ_y = prefactor * shapefactor * integy\n",
    "    \n",
    "    return (DeltaQ_x, DeltaQ_y)\n",
    "\n",
    "calc_tune_spread = np.vectorize(lambda tst_inputs: calculate_exact_tunespread(tst_data, tst_inputs, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need list\n",
    "\n",
    "# from twiss file:\n",
    "# circumference, betas, dispersions \n",
    "\n",
    "#std_x_div_by_Dx\n",
    "\n",
    "# will likely need this to match stored / timber data with tomo data\n",
    "import time\n",
    "time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(1347517370))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.constants import m_p, m_e\n",
    "r_p = 2.8179403267e-15 *m_e/m_p\n",
    "\n",
    "def tstool_ext_calc_tune_spread(data, inputs, beta=0.91444281513833, gamma=2.4708737618826, f_verbose=False):\n",
    "    \"\"\"Calculates the (maximum) tune shift DeltaQ_x and DeltaQ_y due\n",
    "    to space charge with given optics parameters provided by\n",
    "    the dictionaries / hash tables <i>inputs</i> and <i>data</i>.\n",
    "    \"\"\"\n",
    "#     std_x_div_by_Dx is the horizontal profile's standard deviation\n",
    "#     divided by the horizontal dispersion at that point. \n",
    "#     Equivalently for the vertical plane with std_y_div_by_Dy.\n",
    "    r = r_p\n",
    "    #beta = inputs[\"beta\"]\n",
    "    #gamma = inputs[\"gamma\"]\n",
    "    n_part = inputs[\"n_part\"]\n",
    "    emit_x = inputs[\"emit_geom_x\"]\n",
    "    emit_y = inputs[\"emit_geom_y\"]\n",
    "    lshape = inputs.get(\"lshape\", 1.) #from tomo\n",
    "    std_x_div_by_Dx = inputs[\"std_x_div_by_Dx\"] #from gaussian fit + optics_file\n",
    "    integx = integy = 0\n",
    "    \n",
    "    ds = np.diff(data[\"s\"])\n",
    "    beta_x = data[\"beta_x\"][:-1] #from optics file\n",
    "    beta_y = data[\"beta_y\"][:-1] #from optics file\n",
    "    d_x = data[\"d_x\"][:-1] #from optics file\n",
    "\n",
    "    # dispersion only scales the convolution (joint PDF of x = x_beta + D_x * dp)\n",
    "    sqx = std_x_div_by_Dx * d_x\n",
    "    sqy = np.sqrt(emit_y * beta_y)\n",
    "    integx = np.sum(beta_x * ds / (sqx * (sqx + sqy)))\n",
    "    integy = np.sum(beta_y * ds / (sqy * (sqx + sqy)))\n",
    "    \n",
    "    prefactor = r * n_part / (2.0 * np.pi * beta**2 * gamma**3)\n",
    "    if inputs.get('coasting', False):\n",
    "        circumference = data[\"s\"][-1]\n",
    "        shapefactor = lshape / circumference\n",
    "    else:\n",
    "        # assume Gaussian shape\n",
    "        shapefactor = lshape / (np.sqrt(2.0 * np.pi) * inputs[\"sig_z\"])\n",
    "    DeltaQ_x = prefactor * shapefactor * integx\n",
    "    DeltaQ_y = prefactor * shapefactor * integy\n",
    "\n",
    "    return (DeltaQ_x, DeltaQ_y)\n",
    "\n",
    "def calc_dQ_tunespreadtool(\n",
    "        data, mass=m_p, beta=beta, gamma=gamma, \n",
    "        ws_beta_y=11.83, ws_disp_x=2.3, use_long_gauss=False,\n",
    "        extended_Dx_part=False, use_std_not_core_sigx=True):\n",
    "    '''Return the maximum transverse Gaussian tune shift.\n",
    "    If use_long_gauss is True, the formula assuming\n",
    "    Gaussian longitudinal shape is used instead of\n",
    "    evaluating the real maximum line density.\n",
    "    '''\n",
    "    \n",
    "    # Need to select the correct twiss file depending on the tune\n",
    "    with open('/home/oeftiger/cern/git/oeftiger/tunespreadtool/PS-inj2GeV-data.tfs', 'r') as f:\n",
    "        for _ in xrange(52): f.readline()\n",
    "        labels = f.readline().split()[1:]\n",
    "        f.readline()\n",
    "        twiss_data = np.genfromtxt(f, unpack=True)\n",
    "    \n",
    "    with open('/home/oeftiger/cern/git/oeftiger/tunespreadtool/PS-inj2GeV-s.tfs', 'r') as f:\n",
    "        for _ in xrange(52): f.readline()\n",
    "        labels += f.readline().split()[1:]\n",
    "        f.readline()\n",
    "        f_twiss_data = np.genfromtxt(f, unpack=True)\n",
    "    \n",
    "    twiss_data = np.vstack((twiss_data, f_twiss_data))\n",
    "        \n",
    "    tst_data = {var.lower(): array for var, array in zip(labels, twiss_data)}\n",
    "    tst_data['beta_x'] = tst_data.pop('beta11')\n",
    "    tst_data['beta_y'] = tst_data.pop('beta22')\n",
    "    tst_data['d_x'] = tst_data.pop('disp1') * beta\n",
    "    tst_data['d_y'] = tst_data.pop('disp2') * beta\n",
    "    circumference = tst_data['s'][-1]\n",
    "    \n",
    "    # tunespreadtool, attention: need to use numpy arrays\n",
    "    def make_tst_inputs(intensity, deltap, emit_geo_x, emit_geo_y, \n",
    "                        sig_z, std_x_div_by_Dx):\n",
    "        return dict(\n",
    "            mass=(mass * c**2/e * 1e-9),\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            sig_z=sig_z,\n",
    "            n_part=intensity,\n",
    "            deltap=deltap,\n",
    "            emit_geom_x=emit_geo_x,\n",
    "            emit_geom_y=emit_geo_y,\n",
    "            coasting=not use_long_gauss,\n",
    "            n_charges_per_part=1,\n",
    "            std_x_div_by_Dx=std_x_div_by_Dx,\n",
    "        )\n",
    "    make_tst_inputs = np.vectorize(make_tst_inputs)\n",
    "    if not extended_Dx_part:\n",
    "        calc_tune_spread = np.vectorize(\n",
    "            lambda tst_inputs: tunespread.calc_tune_spread(\n",
    "                tst_data, tst_inputs, False) \n",
    "        )\n",
    "    else:\n",
    "        calc_tune_spread = np.vectorize(\n",
    "            lambda tst_inputs: tstool_ext_calc_tune_spread(\n",
    "                tst_data, tst_inputs, False) \n",
    "        )\n",
    "    \n",
    "    lambda_max_norm = data['peakcurrent'][1] / (beta * c * e) / data['eperimage'][0]\n",
    "    \n",
    "    tst_inputs = make_tst_inputs(\n",
    "        np.array(data['intensity'][0]),\n",
    "        data['deltap'][1],\n",
    "        data['emit_x'][0] / (beta*gamma),\n",
    "        data['std_y'][0]**2 / ws_beta_y,\n",
    "#         data['emit_y'][0] / (beta*gamma)\n",
    "        data['bunchlength'][1]*1e-9 / 4 * beta * c,\n",
    "#         data['std_x'][0] / ws_disp_x,\n",
    "        data['std_x' if use_std_not_core_sigx else 'core_sig_x'][0] / ws_disp_x,\n",
    "    )\n",
    "    \n",
    "    if use_long_gauss:\n",
    "        tst_dQ_x, tst_dQ_y = calc_tune_spread(tst_inputs)\n",
    "    else:\n",
    "        tst_dQ_x, tst_dQ_y = (circumference * lambda_max_norm * \n",
    "                              calc_tune_spread(tst_inputs))\n",
    "    return tst_dQ_x, tst_dQ_y\n",
    "\n",
    "for data in (data_hollow, data_gauss):\n",
    "    data['dQx'], data['dQy'] = calc_dQ_tunespreadtool(data)\n",
    "    data['dQx_fakeGauss'], data['dQy_fakeGauss'] = \\\n",
    "        calc_dQ_tunespreadtool(data, use_long_gauss=True)\n",
    "\n",
    "    data['dQx_disp'], data['dQy_disp'] = calc_dQ_tunespreadtool(\n",
    "        data, extended_Dx_part=True)\n",
    "    data['dQx_disp_fakeGauss'], data['dQy_disp_fakeGauss'] = \\\n",
    "        calc_dQ_tunespreadtool(data, extended_Dx_part=True, \n",
    "                               use_long_gauss=True)\n",
    "        \n",
    "    data['dQx_disp_core'], data['dQy_disp_core'] = calc_dQ_tunespreadtool(\n",
    "        data, extended_Dx_part=True, use_std_not_core_sigx=False)\n",
    "    data['dQx_disp_core_fakeGauss'], data['dQy_disp_core_fakeGauss'] = \\\n",
    "        calc_dQ_tunespreadtool(data, extended_Dx_part=True, \n",
    "                               use_long_gauss=True, use_std_not_core_sigx=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
